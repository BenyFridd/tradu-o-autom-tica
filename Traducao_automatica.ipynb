{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "GSa_nx-9ruEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a47045-e8a0-4eb8-f8e6-6bd6c23b770d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: d2l in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.11/dist-packages (from d2l) (1.0.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (from d2l) (1.23.5)\n",
            "Requirement already satisfied: matplotlib==3.7.2 in /usr/local/lib/python3.11/dist-packages (from d2l) (3.7.2)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.6 in /usr/local/lib/python3.11/dist-packages (from d2l) (0.1.6)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.11/dist-packages (from d2l) (2.31.0)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.11/dist-packages (from d2l) (2.0.3)\n",
            "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.11/dist-packages (from d2l) (1.10.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->d2l) (6.5.7)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->d2l) (5.6.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->d2l) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->d2l) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->d2l) (6.17.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->d2l) (7.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l) (11.1.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l) (2.8.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.11/dist-packages (from matplotlib-inline==0.1.6->d2l) (5.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3->d2l) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3->d2l) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0->d2l) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0->d2l) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0->d2l) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0->d2l) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.2->d2l) (1.17.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (6.1.12)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (6.4.2)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.0.13)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-console->jupyter==1.0.0->d2l) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from jupyter-console->jupyter==1.0.0->d2l) (2.18.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter==1.0.0->d2l) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.1)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (3.1.6)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (1.5.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter==1.0.0->d2l) (23.1.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.2.0)\n",
            "Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from qtconsole->jupyter==1.0.0->d2l) (2.4.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter==1.0.0->d2l) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter==1.0.0->d2l) (1.4.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (4.9.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter==1.0.0->d2l) (4.3.7)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (0.2.4)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l) (4.23.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->d2l) (0.2.13)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado>=0.8.3->notebook->jupyter==1.0.0->d2l) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook->jupyter==1.0.0->d2l) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l) (4.12.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l) (0.23.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install d2l\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Importar Bibliotecas e Configurar o Dataset"
      ],
      "metadata": {
        "id": "H_a9CqXZJT4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy -y\n",
        "!pip install numpy==1.21.6\n"
      ],
      "metadata": {
        "id": "Ek3t9j9cJ-3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e8152bd-7180-4b75-8d3b-1ac41f8b9ebc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.23.5\n",
            "Uninstalling numpy-1.23.5:\n",
            "  Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement numpy==1.21.6 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.25.0, 1.25.1, 1.25.2, 1.26.0, 1.26.1, 1.26.2, 1.26.3, 1.26.4, 2.0.0, 2.0.1, 2.0.2, 2.1.0rc1, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0rc1, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.2.4)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for numpy==1.21.6\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from d2l import torch as d2l  # Use a versão PyTorch do d2l\n",
        "\n",
        "# Configurar o dataset do Projeto Tatoeba\n",
        "d2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip',\n",
        "                           '94646ad1522d915e7b0f9296181140edcf86a4f5')\n"
      ],
      "metadata": {
        "id": "Pb3CnuvcJUn9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Download e Pré‑processamento do Conjunto de Dados"
      ],
      "metadata": {
        "id": "rS5OoHiRJV6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data_nmt():\n",
        "    \"\"\"Carrega o conjunto de dados inglês-francês.\"\"\"\n",
        "    data_dir = d2l.download_extract('fra-eng')\n",
        "    with open(os.path.join(data_dir, 'fra.txt'), 'r') as f:\n",
        "        return f.read()\n",
        "\n",
        "raw_text = read_data_nmt()\n",
        "print(raw_text[:75])\n"
      ],
      "metadata": {
        "id": "ZtKsaS76JWdG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a959cae0-6842-4c4f-ec77-442faa61dae2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ../data/fra-eng.zip from http://d2l-data.s3-accelerate.amazonaws.com/fra-eng.zip...\n",
            "Go.\tVa !\n",
            "Hi.\tSalut !\n",
            "Run!\tCours !\n",
            "Run!\tCourez !\n",
            "Who?\tQui ?\n",
            "Wow!\tÇa alors !\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pré‑processamento e Tokenização"
      ],
      "metadata": {
        "id": "kaalfrJvJXiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_nmt(text):\n",
        "    \"\"\"Pré-processa o conjunto de dados: substitui espaços não separáveis, converte para minúsculas\n",
        "    e insere espaços entre palavras e pontuações.\"\"\"\n",
        "    def no_space(char, prev_char):\n",
        "        return char in set(',.!?') and prev_char != ' '\n",
        "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
        "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
        "           for i, char in enumerate(text)]\n",
        "    return ''.join(out)\n",
        "\n",
        "text = preprocess_nmt(raw_text)\n",
        "print(text[:80])\n",
        "\n",
        "def tokenize_nmt(text, num_examples=None):\n",
        "    \"\"\"Tokeniza o conjunto de dados em pares de frases (origem e destino).\"\"\"\n",
        "    source, target = [], []\n",
        "    for i, line in enumerate(text.split('\\n')):\n",
        "        if num_examples and i > num_examples:\n",
        "            break\n",
        "        parts = line.split('\\t')\n",
        "        if len(parts) == 2:\n",
        "            source.append(parts[0].split(' '))\n",
        "            target.append(parts[1].split(' '))\n",
        "    return source, target\n",
        "\n",
        "source, target = tokenize_nmt(text)\n",
        "print(source[:6], target[:6])\n"
      ],
      "metadata": {
        "id": "DF6csgq3JYE-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e1171b-1eb3-42d2-9068-f553eb7f8b4a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "go .\tva !\n",
            "hi .\tsalut !\n",
            "run !\tcours !\n",
            "run !\tcourez !\n",
            "who ?\tqui ?\n",
            "wow !\tça alors !\n",
            "[['go', '.'], ['hi', '.'], ['run', '!'], ['run', '!'], ['who', '?'], ['wow', '!']] [['va', '!'], ['salut', '!'], ['cours', '!'], ['courez', '!'], ['qui', '?'], ['ça', 'alors', '!']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construção do Vocabulário e Preparação dos Dados"
      ],
      "metadata": {
        "id": "zxk0SFv8JZLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criação dos vocabulários para os dois idiomas\n",
        "src_vocab = d2l.Vocab(source, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
        "tgt_vocab = d2l.Vocab(target, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
        "print(\"Tamanho do vocabulário de origem:\", len(src_vocab))\n",
        "print(\"Tamanho do vocabulário de destino:\", len(tgt_vocab))\n",
        "\n",
        "def truncate_pad(line, num_steps, padding_token):\n",
        "    \"\"\"Trunca ou preenche uma sequência para ter comprimento fixo.\"\"\"\n",
        "    if len(line) > num_steps:\n",
        "        return line[:num_steps]\n",
        "    return line + [padding_token] * (num_steps - len(line))\n",
        "\n",
        "def build_array_nmt(lines, vocab, num_steps):\n",
        "    \"\"\"Converte sequências de texto em arrays, adicionando o token <eos> e aplicando truncamento/padding.\"\"\"\n",
        "    lines = [vocab[l] for l in lines]\n",
        "    lines = [l + [vocab['<eos>']] for l in lines]\n",
        "    array = [truncate_pad(l, num_steps, vocab['<pad>']) for l in lines]\n",
        "    valid_len = [sum(token != vocab['<pad>'] for token in l) for l in array]\n",
        "    return torch.tensor(array), torch.tensor(valid_len)\n",
        "\n",
        "def load_data_nmt(batch_size, num_steps, num_examples=600):\n",
        "    \"\"\"Retorna o iterador de dados e os vocabulários para a tradução automática.\"\"\"\n",
        "    text = preprocess_nmt(read_data_nmt())\n",
        "    source, target = tokenize_nmt(text, num_examples)\n",
        "    src_vocab = d2l.Vocab(source, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
        "    tgt_vocab = d2l.Vocab(target, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
        "    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n",
        "    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\n",
        "    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
        "    return d2l.load_array(data_arrays, batch_size), src_vocab, tgt_vocab\n",
        "\n",
        "batch_size = 2\n",
        "num_steps = 8\n",
        "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size, num_steps, num_examples=600)\n",
        "\n",
        "for X, X_valid_len, Y, Y_valid_len in train_iter:\n",
        "    print('X:', X)\n",
        "    print('Comprimentos válidos de X:', X_valid_len)\n",
        "    print('Y:', Y)\n",
        "    print('Comprimentos válidos de Y:', Y_valid_len)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "7NWWW0UKJZyb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f51c6682-829e-44f3-b9f9-b902756248ee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho do vocabulário de origem: 10012\n",
            "Tamanho do vocabulário de destino: 17851\n",
            "X: tensor([[ 58, 158,   2,   4,   5,   5,   5,   5],\n",
            "        [ 59,  75,   2,   4,   5,   5,   5,   5]])\n",
            "Comprimentos válidos de X: tensor([4, 4])\n",
            "Y: tensor([[188,  43, 179,   2,   4,   5,   5,   5],\n",
            "        [155, 197, 105, 115,   2,   4,   5,   5]])\n",
            "Comprimentos válidos de Y: tensor([5, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição do Modelo Seq2Seq (Encoder, Decoder e Modelo Completo)"
      ],
      "metadata": {
        "id": "0odWT1ZHJbIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder: codifica a sequência de entrada\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, x, valid_len):\n",
        "        # x: (batch, seq_len)\n",
        "        embedded = self.embedding(x)\n",
        "        # Usa pack_padded_sequence para ignorar os tokens de padding\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, valid_len.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        _, hidden = self.gru(packed)\n",
        "        return hidden\n",
        "\n",
        "# Decoder: gera a sequência de saída\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        # x: (batch, 1) – o token atual de entrada\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        output = self.fc(output.squeeze(1))  # (batch, vocab_size)\n",
        "        return output, hidden\n",
        "\n",
        "# Modelo Seq2Seq que une Encoder e Decoder\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, src_valid_len, tgt, teacher_forcing_ratio=0.5):\n",
        "        batch_size, tgt_len = tgt.size()\n",
        "        vocab_size = self.decoder.embedding.num_embeddings\n",
        "        outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(self.device)\n",
        "\n",
        "        hidden = self.encoder(src, src_valid_len)\n",
        "        # O primeiro token do target (<bos>) é a entrada inicial do decoder\n",
        "        input = tgt[:, 0].unsqueeze(1)  # shape (batch, 1)\n",
        "\n",
        "        for t in range(1, tgt_len):\n",
        "            output, hidden = self.decoder(input, hidden)\n",
        "            outputs[:, t] = output\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1).unsqueeze(1)\n",
        "            input = tgt[:, t].unsqueeze(1) if teacher_force else top1\n",
        "        return outputs\n",
        "\n",
        "# Configurar dispositivo\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hiperparâmetros\n",
        "ENC_EMB_DIM = 32\n",
        "DEC_EMB_DIM = 32\n",
        "HID_DIM = 64\n",
        "\n",
        "encoder = Encoder(len(src_vocab), ENC_EMB_DIM, HID_DIM).to(device)\n",
        "decoder = Decoder(len(tgt_vocab), DEC_EMB_DIM, HID_DIM).to(device)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n"
      ],
      "metadata": {
        "id": "Q1XVswN6Jbuc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Treinamento do Modelo"
      ],
      "metadata": {
        "id": "pEd39kR4Jdo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de perda ignorando os tokens de padding (<pad>)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab['<pad>'])\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "def train_epoch(model, data_iter, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    correct_tokens = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for src, src_valid_len, tgt, _ in data_iter:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(src, src_valid_len, tgt, teacher_forcing_ratio=0.5)\n",
        "        # outputs: (batch, tgt_len, vocab_size) e tgt: (batch, tgt_len)\n",
        "        output_dim = outputs.shape[-1]\n",
        "        outputs_reshaped = outputs[:, 1:].reshape(-1, output_dim)\n",
        "        tgt_reshaped = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(outputs_reshaped, tgt_reshaped)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Cálculo da acurácia dos tokens (ignorando padding)\n",
        "        predictions = outputs_reshaped.argmax(1)\n",
        "        mask = tgt_reshaped != tgt_vocab['<pad>']\n",
        "        correct_tokens += (predictions[mask] == tgt_reshaped[mask]).sum().item()\n",
        "        total_tokens += mask.sum().item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(data_iter)\n",
        "    accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "EPOCHS = 20\n",
        "CLIP = 1.0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    loss, accuracy = train_epoch(model, train_iter, optimizer, criterion, CLIP)\n",
        "    print(f'Época {epoch+1}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "_z1OIghUJeWw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "045423d7-97f5-4f02-df18-3547b63692ea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 1, Loss: 0.8841, Accuracy: 0.7608\n",
            "Época 2, Loss: 0.7650, Accuracy: 0.7830\n",
            "Época 3, Loss: 0.7564, Accuracy: 0.7866\n",
            "Época 4, Loss: 0.6718, Accuracy: 0.8062\n",
            "Época 5, Loss: 0.6000, Accuracy: 0.8289\n",
            "Época 6, Loss: 0.5616, Accuracy: 0.8268\n",
            "Época 7, Loss: 0.5344, Accuracy: 0.8438\n",
            "Época 8, Loss: 0.4787, Accuracy: 0.8552\n",
            "Época 9, Loss: 0.4483, Accuracy: 0.8624\n",
            "Época 10, Loss: 0.4131, Accuracy: 0.8680\n",
            "Época 11, Loss: 0.3844, Accuracy: 0.8727\n",
            "Época 12, Loss: 0.3722, Accuracy: 0.8742\n",
            "Época 13, Loss: 0.3449, Accuracy: 0.8861\n",
            "Época 14, Loss: 0.3389, Accuracy: 0.8840\n",
            "Época 15, Loss: 0.2773, Accuracy: 0.8974\n",
            "Época 16, Loss: 0.2928, Accuracy: 0.8887\n",
            "Época 17, Loss: 0.3108, Accuracy: 0.8923\n",
            "Época 18, Loss: 0.2918, Accuracy: 0.8959\n",
            "Época 19, Loss: 0.2763, Accuracy: 0.8954\n",
            "Época 20, Loss: 0.2861, Accuracy: 0.8876\n"
          ]
        }
      ]
    }
  ]
}